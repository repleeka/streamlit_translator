{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('tawra_5k.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENGLISH</th>\n",
       "      <th>TARAON/TAWRA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Run!</td>\n",
       "      <td>chow na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>tamung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Help!</td>\n",
       "      <td>di brungna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>na naa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>kalyuna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ENGLISH  TARAON/TAWRA\n",
       "0     Run!      chow na\n",
       "1    Fire!       tamung\n",
       "2    Help!   di brungna\n",
       "3    Stop!       na naa\n",
       "4    Wait!      kalyuna"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_file = data['ENGLISH ']\n",
    "tawra_file = data['TARAON/TAWRA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_file.to_csv('english.txt',index=False)\n",
    "tawra_file.to_csv('tawra.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the corpus for combined spm model\n",
    "data.to_csv('combined.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parallel_data(combined_file):\n",
    "    \"\"\"\n",
    "    Load parallel data from English and Tawra files.\n",
    "    Args:\n",
    "        combined_file (str): The file path to the combined sentences.\n",
    "    Returns:\n",
    "        tuple: A combined file containing English sentences and Tawra sentences.\n",
    "    \"\"\"\n",
    "    # Read English sentences from the file\n",
    "    with open(combined_file, 'r', encoding='utf-8') as f:\n",
    "        combined_sentences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    return combined_sentences\n",
    "\n",
    "# Example usage:\n",
    "combined_sentences = load_parallel_data('combined.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the vocabulary size for the SentencePiece model\n",
    "vocab_size = 1000\n",
    "\n",
    "def train_sentencepiece_model(data, model_prefix, vocab_size=vocab_size):\n",
    "    \"\"\"\n",
    "    Train a SentencePiece model on the given data.\n",
    "    Args:\n",
    "        data (str): The file path to the input data.\n",
    "        model_prefix (str): The prefix for the output model files.\n",
    "        vocab_size (int): The size of the vocabulary for the model. Default is 6800.\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.train(input=data, model_prefix=model_prefix, vocab_size=vocab_size)\n",
    "\n",
    "# Train SentencePiece model for COmbined texts\n",
    "train_sentencepiece_model('combined.txt', 'eng_taw', vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentencepiece_model(model_prefix):\n",
    "    \"\"\"\n",
    "    Load a trained SentencePiece model.\n",
    "    Args:\n",
    "        model_prefix (str): The prefix for the SentencePiece model files.\n",
    "    Returns:\n",
    "        spm.SentencePieceProcessor: The loaded SentencePiece model.\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f\"{model_prefix}.model\")\n",
    "    return sp\n",
    "\n",
    "# Load SentencePiece models\n",
    "combined_sp = load_sentencepiece_model('eng_taw')\n",
    "# tawra_sp = load_sentencepiece_model('tawra')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data, sp):\n",
    "    \"\"\"\n",
    "    Build a vocabulary from tokenized data using a SentencePiece model.\n",
    "    Args:\n",
    "        data (list): A list of sentences to tokenize.\n",
    "        sp (spm.SentencePieceProcessor): A SentencePiece model to tokenize the sentences.\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are tokens and values are their respective frequencies.\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in data:\n",
    "        tokens = sp.encode(sentence, out_type=str)\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                vocab[token] += 1\n",
    "            else:\n",
    "                vocab[token] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies for English and Tawra\n",
    "combined_vocab = build_vocab(combined_sentences, combined_sp)\n",
    "# tagin_vocab = build_vocab(tawra_sentences, tawra_sp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
